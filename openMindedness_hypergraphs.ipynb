{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from ash_model import ASH\n",
    "import openMindednessFunctions as omf\n",
    "from ash_model import readwrite as io\n",
    "import json\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(datasetname):\n",
    "\n",
    "    h = io.read_ash_from_json(f'datasets/in/{datasetname}/{datasetname}.json')\n",
    "\n",
    "    timestamps = {'2017-01-01_2017-07-01':0, '2017-07-01_2018-01-01':1, '2018-01-01_2018-07-01':2, '2018-07-01_2019-01-01':3, '2019-01-01_2019-07-01':4}\n",
    "\n",
    "    t2node2attrs = dict()\n",
    "    for filename in os.listdir(f'datasets/in/{datasetname}'):\n",
    "        if filename.endswith('nodelist.csv'):\n",
    "            t = filename.split('_')\n",
    "            t = t[1]+'_'+t[2]\n",
    "            if t not in t2node2attrs:\n",
    "                t2node2attrs[timestamps[t]] = dict()\n",
    "            file = open(f'datasets/in/{datasetname}/'+filename, 'r')\n",
    "            csvreader = csv.reader(file)\n",
    "            header = []\n",
    "            header = next(csvreader)\n",
    "            rows = []\n",
    "            for row in csvreader:\n",
    "                rows.append(row)\n",
    "            file.close()\n",
    "            for row in rows:\n",
    "                t2node2attrs[timestamps[t]][int(row[0])] = {'opinion': round(float(row[1]), 2), 'cluster':row[2]}\n",
    "\n",
    "    # with open('datasets/in/guncontrol/'+'guncontrol.json', 'r') as file:\n",
    "    #     h = json.load(file)\n",
    "\n",
    "    t2he2attrs = dict()\n",
    "\n",
    "    for tid in [0, 1, 2, 3, 4]:\n",
    "        t2he2attrs[tid] = dict()\n",
    "        for he in (h.get_hyperedge_id_set(tid=tid)):\n",
    "            t2he2attrs[tid][he] = dict()\n",
    "            nodes = h.get_hyperedge_nodes(he)\n",
    "            for node in nodes:\n",
    "                t2he2attrs[tid][he][node] = t2node2attrs[tid][node]['opinion']\n",
    "\n",
    "\n",
    "    t2node2avgs = dict()\n",
    "\n",
    "    for tid in [0, 1, 2, 3, 4]:\n",
    "        t2node2avgs[tid] = dict()\n",
    "        for u in h.get_node_set(tid=tid):\n",
    "            if (tid+1) in h.get_node_presence(node=u):\n",
    "                t2node2avgs[tid][u] = dict()\n",
    "                t2node2avgs[tid][u]['avgs'] = dict()\n",
    "                t2node2avgs[tid][u]['stds'] = dict()\n",
    "                #posso calcolare open mindedness\n",
    "                hes = h.get_star(node=u, tid=tid)\n",
    "                for he in hes:\n",
    "                    nodes = t2he2attrs[tid][he]\n",
    "                    ops = []\n",
    "                    for node in nodes:\n",
    "                        if node != u:\n",
    "                            ops.append(nodes[node])\n",
    "                    avg, std = np.average(np.array(ops)), np.std(np.array(ops))\n",
    "                    t2node2avgs[tid][u]['avgs'][he] = avg\n",
    "                    t2node2avgs[tid][u]['stds'][he] = std \n",
    "    \n",
    "    return h, t2node2attrs, t2node2avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimation(opvt, opvt1, sorted_vals):\n",
    "    errs = []\n",
    "    estimated_opinions = []\n",
    "    est_opvt1=opvt\n",
    "    for oput in sorted_vals:\n",
    "        est_opvt1 = (est_opvt1 + oput)/2\n",
    "        err = abs(est_opvt1 - opvt1)\n",
    "        estimated_opinions.append(est_opvt1)\n",
    "        errs.append(err)\n",
    "    try:\n",
    "        i = len(errs) - 1 - errs[::-1].index(min(errs))\n",
    "        last_op = sorted_vals[i]\n",
    "        cb = abs(last_op - opvt) \n",
    "    \n",
    "        if errs[i] < abs(opvt-opvt1):\n",
    "            return cb, errs[i], estimated_opinions[i]        \n",
    "        else:\n",
    "            return 0.0, abs(opvt-opvt1), opvt\n",
    "    except:\n",
    "        return -1.0, abs(opvt-opvt1), opvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset():\n",
    "    timestamps = {0:'2017-01-01_2017-07-01', 1:'2017-07-01_2018-01-01', 2:'2018-01-01_2018-07-01', 3:'2018-07-01_2019-01-01', 4:'2019-01-01_2019-07-01'}\n",
    "    data = {}\n",
    "    for dataset_name in ['minority', 'guncontrol', 'politics']:\n",
    "        print('doing ', dataset_name)\n",
    "        h, t2node2attrs, t2node2avgs = prepare_data(dataset_name)\n",
    "        '''\n",
    "        per ogni time step mi ritorna un dizionario nodo: {dizionario con delle info}\n",
    "        '''\n",
    "        data[dataset_name] = {}\n",
    "        for t in tqdm.tqdm([0, 1, 2, 3]):\n",
    "            data[dataset_name][timestamps[t]] = {}\n",
    "            t1=t+1\n",
    "            print('there are ', len(h.get_node_set(t)), ' nodes at time ', t)\n",
    "            a = 0\n",
    "            b = 0\n",
    "            c = 0\n",
    "            for v in h.get_node_set(tid=t):\n",
    "                if t1 in h.get_node_presence(node=v) and h.get_star(node=v, tid=t):\n",
    "                    c+=1\n",
    "                    opvt = t2node2attrs[t][v]['opinion']\n",
    "                    opvt1 = t2node2attrs[t1][v]['opinion']\n",
    "                    sortedNeighOps = sorted(t2node2avgs[t][v]['avgs'].values())\n",
    "                    eps, err, estOp = estimation(opvt, opvt1, sortedNeighOps)\n",
    "                    orientation = omf.politicalLeaning(opvt) \n",
    "                    nactiveint = len([op for op in sortedNeighOps if op <= eps])\n",
    "                    data[dataset_name][timestamps[t]][v] = {'dataset':dataset_name, 'opt':opvt, 'opt1': opvt1, 'neighbors':list(t2node2avgs[t][v]['avgs'].keys()), \n",
    "                                                    'opChange':opvt1-opvt, 'homophily':np.average(np.array(list(t2node2avgs[t][v]['stds'].values()))), 'orientation':orientation, \n",
    "                                                    'neighborsOps':sortedNeighOps, 'error':err, 'estOp':estOp, \n",
    "                                                    'openMindedness':eps, 'nactiveint':nactiveint}    \n",
    "                if not t1 in h.get_node_presence(node=v): a+=1\n",
    "                if not h.get_star(v, t): b+=1\n",
    "            print('we were able to estimate ', c, ' values at time ', t)\n",
    "            print(a, b)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing  minority\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:00<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  1204  nodes at time  0\n",
      "we were able to estimate  195  values at time  0\n",
      "997 1204\n",
      "there are  1221  nodes at time  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we were able to estimate  208  values at time  1\n",
      "1001 1221\n",
      "there are  1372  nodes at time  2\n",
      "we were able to estimate  222  values at time  2\n",
      "1137 314\n",
      "there are  1392  nodes at time  3\n",
      "we were able to estimate  214  values at time  3\n",
      "1162 783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing  guncontrol\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████████████████                                                               | 1/4 [00:00<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  990  nodes at time  0\n",
      "we were able to estimate  138  values at time  0\n",
      "845 990\n",
      "there are  1051  nodes at time  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:00<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we were able to estimate  148  values at time  1\n",
      "892 1051\n",
      "there are  1281  nodes at time  2\n",
      "we were able to estimate  166  values at time  2\n",
      "1106 301\n",
      "there are  1229  nodes at time  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we were able to estimate  162  values at time  3\n",
      "1054 713\n",
      "doing  politics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:00<00:00, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  1204  nodes at time  0\n",
      "we were able to estimate  162  values at time  0\n",
      "1022 1204\n",
      "there are  1101  nodes at time  1\n",
      "we were able to estimate  167  values at time  1\n",
      "895 1101\n",
      "there are  1089  nodes at time  2\n",
      "we were able to estimate  162  values at time  2\n",
      "903 283\n",
      "there are  1044  nodes at time  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 13.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we were able to estimate  164  values at time  3\n",
      "861 679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = createDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'openMindednessData_h.pickle', 'wb') as ofile:\n",
    "    pickle.dump(data, ofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
